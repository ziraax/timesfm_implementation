{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCfBUM2VIppYJ7T7d2Tjxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziraax/timesfm_torch/blob/main/TimesFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook attend to replicate the model presented in the paper : [A DECODER-ONLY FOUNDATION MODEL FOR TIME-SERIES FORECASTING](https://arxiv.org/pdf/2310.10688) from **Google Research** using **Pytorch**.\n",
        "Note that everything here is granted as-is, and without any guarantee.\n",
        "\n"
      ],
      "metadata": {
        "id": "KIsQJuUNfz5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary of the Paper**\n",
        "\n",
        "The paper, titled \"A Decoder-Only Foundation Model for Time-Series Forecasting,\" presents TimesFM, a foundation model designed for time-series forecasting. This model leverages recent advancements in large language models (LLMs) for NLP to build a versatile time-series forecasting model that performs well across various datasets and scenarios without the need for fine-tuning or dataset-specific training.\n",
        "\n",
        "## **Key Components**\n",
        "\n",
        "### Motivation and Objective:\n",
        "  - Goal: Develop a zero-shot time-series forecasting model that performs competitively with state-of-the-art supervised models on unseen datasets.\n",
        "  - Challenges: Unlike NLP, time-series data lacks a standardized vocabulary or grammar, and there is limited availability of large-scale public time-series datasets.\n",
        "\n",
        "### Model Design:\n",
        "  - Architecture: Decoder-only transformer model inspired by language models but adapted for time-series data.\n",
        "  - Patching: Time-series data is broken into patches, akin to tokens in NLP models, to manage long sequences and improve model efficiency.\n",
        "  - Decoder-Only Approach: The model uses a decoder-only architecture for efficient training and prediction.\n",
        "\n",
        "### Training Data:\n",
        "  - Dataset Composition: Combines real-world (e.g., web search queries, Wikipedia page visits) and synthetic time-series data to create a large and diverse training corpus.\n",
        "\n",
        "### Model Characteristics:\n",
        "  - Flexibility: Capable of handling varying context lengths, prediction horizons, and time granularities.\n",
        "  - Efficiency: Despite being smaller in parameter size and data volume compared to typical LLMs, TimesFM achieves competitive zero-shot performance.\n",
        "\n",
        "### Comparison with Existing Models:\n",
        "  - Performance: Demonstrates superior zero-shot performance compared to LLM-based forecasters (e.g., GPT-3, LLaMA-2) at a fraction of the cost.\n",
        "  - State-of-the-Art: Matches or nearly matches the accuracy of best-in-class supervised models on various forecasting tasks.\n",
        "\n",
        "# **Detailed Explanation**\n",
        "  \n",
        "### Introduction and Background:\n",
        "  - Highlights the ubiquity and importance of time-series data in domains like retail, finance, healthcare, and more.\n",
        "  - Discusses the rise of deep learning models in time-series forecasting and their advantages over classical statistical methods like ARIMA and GARCH.\n",
        "  - Draws parallels with the success of large language models in NLP to motivate the development of a time-series foundation model.\n",
        "\n",
        "### Related Work:\n",
        "  - Reviews previous approaches in time-series forecasting, including local univariate models, global univariate models, and global multivariate models.\n",
        "  - Notes recent attempts to use pretrained LLMs for time-series forecasting, emphasizing the novelty and efficiency of TimesFM in this context.\n",
        "\n",
        "### Model Architecture:\n",
        "  - Input Layer: Processes time-series into patches, each processed into a vector by a residual block.\n",
        "  - Stacked Transformer Layers: Utilizes multi-head self-attention with causal attention to ensure each output token attends to only preceding tokens.\n",
        "  - Output Layer: Maps the encoded information into future time-series predictions.\n",
        "  - Loss Function: Uses Mean Squared Error (MSE) for point forecasting, with flexibility for probabilistic forecasting if needed.\n",
        "\n",
        "### Training Strategy:\n",
        "  - Describes the use of mini-batch gradient descent and a unique patch masking strategy to ensure the model learns across varying context lengths.\n",
        "\n",
        "### Experiments and Results:\n",
        "  - Dataset Variety: Evaluates the model on diverse unseen datasets, demonstrating robust zero-shot performance.\n",
        "  - Performance Metrics: Compares TimesFM's accuracy to supervised models, showing close or superior results.\n",
        "\n",
        "### Conclusion:\n",
        "  - Summarizes the effectiveness of TimesFM as a practical, efficient solution for time-series forecasting.\n",
        "  - Suggests potential future directions, including further scaling and fine-tuning for specific applications.\n",
        "\n",
        "### Implications and Benefits\n",
        "  - Efficiency: TimesFM provides a highly efficient alternative to traditional supervised models, significantly reducing training and computational costs.\n",
        "  - Versatility: The model's ability to perform well across different datasets and scenarios without additional training makes it highly versatile for real-world applications.\n",
        "  - Foundation for Future Work: Sets a new benchmark for zero-shot time-series forecasting, paving the way for further research and development in this area.\n",
        "\n",
        "## **More on the model architecture**\n",
        "\n",
        "TimesFM leverages a decoder-only transformer architecture, which is inspired by large language models (LLMs) but adapted for the unique characteristics of time-series data. The key components of the architecture include the input processing, transformer layers, and the output generation.\n",
        "\n",
        "### Detailed Components\n",
        "\n",
        "  1. **Input Processing:**\n",
        "\n",
        "    *Time-Series Patching*:\n",
        "        The continuous time-series data is divided into fixed-size patches. Each patch represents a segment of the time-series data.\n",
        "        This approach is akin to tokenizing text in NLP models, where patches serve as the basic units of input.\n",
        "\n",
        "    *Embedding*:\n",
        "        Each patch is embedded into a higher-dimensional space using a linear embedding layer. This transforms the raw time-series values into vectors that the model can process.\n",
        "        The embedding layer is followed by a residual block that helps stabilize the training and enhances the model's ability to capture complex patterns.\n",
        "\n",
        "  2. **Transformer Layers:**\n",
        "\n",
        "    *Multi-Head Self-Attention*:\n",
        "        Self-attention allows the model to weigh the importance of different patches when making predictions. In the context of time-series, it helps the model understand dependencies across different time steps.\n",
        "        Multi-head attention involves multiple attention mechanisms running in parallel, providing the model with the ability to capture various aspects of the data's structure.\n",
        "\n",
        "    *Causal Attention*:\n",
        "        Causal attention ensures that each output only depends on the current and past inputs, preventing information leakage from future patches. This is crucial for time-series forecasting, where predictions at a given time should not be influenced by future data.\n",
        "\n",
        "    *Feed-Forward Neural Network*:\n",
        "        Each transformer layer includes a position-wise feed-forward network, applied independently to each position (patch) in the sequence.\n",
        "        The feed-forward network consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "    *Residual Connections and Layer Normalization*:\n",
        "        Residual connections are used around each sub-layer (multi-head attention and feed-forward network) to facilitate gradient flow and prevent vanishing gradients.\n",
        "        Layer normalization is applied to stabilize and speed up training.\n",
        "\n",
        "  3. **Output Generation:**\n",
        "\n",
        "    *Prediction Layer*:\n",
        "        The final layer of the model maps the output from the transformer layers to the predicted time-series values. This is typically a linear transformation.\n",
        "        For point forecasting, the model directly outputs the future values. For probabilistic forecasting, it can output parameters of a probability distribution.\n",
        "\n",
        "    *Loss Function*:\n",
        "        The primary loss function used is Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values. MSE is suitable for point forecasts where the objective is to minimize the prediction error.\n",
        "        The model is flexible enough to accommodate other loss functions for different forecasting objectives, such as probabilistic forecasts.\n",
        "\n",
        "  4. **Advantages of the Architecture**\n",
        "\n",
        "    *Scalability*:\n",
        "        The transformer architecture is highly scalable, allowing the model to handle long time-series by processing patches in parallel. This is essential for efficiently dealing with large datasets.\n",
        "\n",
        "    *Flexibility*:\n",
        "        By using a decoder-only approach, TimesFM can generate predictions iteratively, making it suitable for a variety of forecasting horizons and granularities.\n",
        "        The model's structure allows for handling diverse time-series data without needing dataset-specific customization.\n",
        "\n",
        "    *Efficiency*:\n",
        "        Despite being smaller in size compared to typical LLMs, TimesFM achieves competitive performance due to its efficient use of the transformer architecture and effective training strategies.\n",
        "\n",
        "    *Generalization*:\n",
        "        The use of diverse training data (both real-world and synthetic) enables the model to generalize well to unseen datasets, providing robust zero-shot performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ETidOXOgX_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diagram of the Model Architecture\n",
        "\n",
        "![timesfm.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "J_PUWTLHkWfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Strategy\n",
        "\n",
        "  - Mini-Batch Gradient Descent: The model is trained using mini-batch gradient descent, which is efficient for large datasets and helps in generalization.\n",
        "  - Patch Masking Strategy: During training, a masking strategy is used to ensure the model learns effectively across varying context lengths, preventing overfitting to specific patterns."
      ],
      "metadata": {
        "id": "S6WAkIfPkm35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libs"
      ],
      "metadata": {
        "id": "SPRoFWiGkxgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "oh5oQIE3k1VF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head self-attention layer"
      ],
      "metadata": {
        "id": "di55I_axk-D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorry i comment my code in french, but will be removed to be in english\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "\n",
        "    \"\"\"\n",
        "    d_model : La dimension de l'espace de représentation des embeddings d'entrée\n",
        "    et de sortie.\n",
        "\n",
        "    num_heads : Le nombre de têtes d'attention. Chaque tête d'attention apprend\n",
        "    à capturer différents aspects des dépendances dans la séquence d'entrée.\n",
        "\n",
        "    depth : La dimension de chaque tête d'attention, calculée comme d_model // num_heads.\n",
        "    Ceci assure que la concaténation des sorties de toutes les têtes donne une\n",
        "    représentation de dimension d_model.\n",
        "\n",
        "    Les transformations linéaires wq, wk, et wv sont utilisées pour générer les\n",
        "    matrices de requêtes (queries), de clés (keys), et de valeurs (values)\n",
        "    respectivement. La transformation dense est appliquée après l'opération\n",
        "    d'attention pour obtenir la sortie finale.\n",
        "    \"\"\"\n",
        "\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.depth = d_model // num_heads\n",
        "\n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"\n",
        "    Cette fonction réorganise les dimensions du tenseur x afin de séparer les têtes d'attention :\n",
        "\n",
        "    x.view(batch_size, -1, self.num_heads, self.depth) : Réorganise le tenseur\n",
        "    pour avoir num_heads têtes avec depth dimensions chacune. La nouvelle forme\n",
        "    est [batch_size, seq_length, num_heads, depth].\n",
        "\n",
        "    permute(0, 2, 1, 3) : Permute les dimensions pour obtenir la forme\n",
        "    [batch_size, num_heads, seq_length, depth], ce qui est nécessaire pour\n",
        "    le calcul de l'attention multi-tête.\n",
        "    \"\"\"\n",
        "    x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    \"\"\"\n",
        "    Les transformations linéaires wq, wk, et wv sont appliquées à l'entrée x\n",
        "    pour obtenir les requêtes, clés, et valeurs, qui sont ensuite réorganisées\n",
        "    pour les têtes d'attention.\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    q = self.split_heads(self.wq(x), batch_size)\n",
        "    k = self.split_heads(self.wk(x), batch_size)\n",
        "    v = self.split_heads(self.wv(x), batch_size)\n",
        "\n",
        "    \"\"\"\n",
        "    Les scores d'attention sont calculés en prenant le produit scalaire des\n",
        "    requêtes et des clés, normalisé par la racine carrée de la profondeur.\n",
        "    Cette normalisation aide à stabiliser les gradients lors de l'entraînement.\n",
        "    \"\"\"\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.depth)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores += (mask * -1e9)\n",
        "\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn_weights, v)\n",
        "\n",
        "    output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "    output = self.dense(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "HQrjBmPDlIo4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed-forward layer"
      ],
      "metadata": {
        "id": "yDYRSEjlpbMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(F.relu(self.linear1(x)))\n",
        "    x = self.linear2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "pt0CNH3PpCgX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization & residual layer"
      ],
      "metadata": {
        "id": "oVIMVwSRpjQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormResidual(nn.Module):\n",
        "  def __init__(self, d_model, sublayer):\n",
        "    super(LayerNormResidual, self).__init__()\n",
        "    self.sublayer = sublayer\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, *args, **kwargs):\n",
        "    return x + self.sublayer(self.norm(x), *args, **kwargs)"
      ],
      "metadata": {
        "id": "7EhyqOR5pqPu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Decoder Layer"
      ],
      "metadata": {
        "id": "pIaeCaHqptT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "    super(TransformerDecoderLayer, self).__init__()\n",
        "    self.attention = LayerNormResidual(d_model, MultiHeadAttention(d_model, num_heads))\n",
        "    self.feed_forward = LayerNormResidual(d_model, FeedForward(d_model, d_ff, dropout))\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    x = self.attention(x, mask)\n",
        "    x = self.feed_forward(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "mpaDp7WIpzBc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete model"
      ],
      "metadata": {
        "id": "rkl-d0Hop1H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimesFM(nn.Module):\n",
        "  def __init__(self, input_size, d_model, num_heads, num_layers, d_ff, dropout=0.1):\n",
        "    super(TimesFM, self).__init__()\n",
        "    self.embedding = nn.Linear(input_size, d_model)\n",
        "    self.pos_embedding = nn.Parameter(torch.zeros(1, 5000, d_model))\n",
        "    self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "    self.output_layer = nn.Linear(d_model, input_size)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    seq_length = x.size(1)\n",
        "    x = self.embedding(x)\n",
        "    x += self.pos_embedding[:, :seq_length, :]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = self.output_layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def create_causal_mask(size):\n",
        "  mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
        "  mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "  return mask"
      ],
      "metadata": {
        "id": "YDkZmPPhp3RC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "9i1hpqsBp541"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU acceleration) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFGAuDNvCcS-",
        "outputId": "9071d7b3-4a54-49b7-efb9-f4ade799d4ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequence_windows(data, target, seq_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - seq_length + 1):  # Adjusted loop to include all valid windows\n",
        "        sequences.append(data[i:i+seq_length])\n",
        "        targets.append(target[i+seq_length-1])  # Use the last element as target for each window\n",
        "    return torch.stack(sequences), torch.stack(targets)\n"
      ],
      "metadata": {
        "id": "m-R-PFa3EVnB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, num_epochs):\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    for inputs, targets in data_loader:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      target_shape = outputs.shape  # [batch_size, seq_length, 1]\n",
        "      targets_expanded = targets.unsqueeze(1).expand(-1, target_shape[1], -1)\n",
        "\n",
        "      loss = criterion(outputs, targets_expanded)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "ZMs8Q7aGD5wF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemple"
      ],
      "metadata": {
        "id": "0B-M8lC5qDeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jp8PbZ_1qsIL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetical data\n",
        "data = np.sin(np.linspace(0, 100, 5000))\n",
        "X = np.array([data[i:i+50] for i in range(len(data)-50)])\n",
        "y = np.array([data[i+50] for i in range(len(data)-50)])\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)\n",
        "y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "X_seq, y_seq = create_sequence_windows(X, y, seq_length=50)\n",
        "\n",
        "dataset = TensorDataset(X_seq, y_seq)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = TimesFM(input_size=1, d_model=128, num_heads=8, num_layers=6, d_ff=512)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "train(model, data_loader, criterion, optimizer, num_epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ktnmc_O7qA_L",
        "outputId": "510157a3-98b9-4f19-b656-d7ff5f976517"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (50) must match the size of tensor b (2500) at non-singleton dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-dc5bc028dd72>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-5602b51e4c3e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mtarget_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[0;31m# [batch_size, seq_length, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-65e7995b8bdb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e8985f2d1252>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-fa16d8bfef12>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (2500) at non-singleton dimension 2"
          ]
        }
      ]
    }
  ]
}